<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Virtual Makeup - Apply Earrings | OpenCV</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Virtual Makeup - Apply Earrings" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://teapython.github.io/OpenCV/" />
<meta property="og:url" content="https://teapython.github.io/OpenCV/" />
<meta property="og:site_name" content="OpenCV" />
<script type="application/ld+json">
{"@type":"WebSite","headline":"Virtual Makeup - Apply Earrings","url":"https://teapython.github.io/OpenCV/","name":"OpenCV","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="/OpenCV/assets/css/style.css?v=a2299fb24d442b4111751d863e052f0ec8bd89ff">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Fun OpenCV Projects</h1>
      <h2 class="project-tagline"></h2>
      
        <a href="https://github.com/teapython/OpenCV" class="btn">View on GitHub</a>
      
      
    </section>

    <section class="main-content">
      <h1 id="virtual-makeup---apply-earrings">Virtual Makeup - Apply Earrings</h1>

<p>This is a demonstration of how to use OpenCV and Dlib to apply earrings to a face image, also a project for the OpenCV course: Computer Vision II. For those who love earrings!</p>

<h2 id="the-core-idea">The Core Idea</h2>

<ul>
  <li>Detect landmarks on the face</li>
  <li>Scale the earring images to fit to the face</li>
  <li>Estimate the earlobe locations based on landmarks</li>
  <li>Using earlobe locations to find regions on the face image to be replaced with the earring images</li>
  <li>Alpha blend images of these regions with the earring images</li>
  <li>Replace regions in the face image with the alpha-blended images</li>
</ul>

<h2 id="code">Code</h2>

<p>Import libraries and set up image display parameters.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import cv2, dlib
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
</code></pre></div></div>

<p>Read the original image without makeup. It was generated completely by AI from <a href="https://generated.photos">https://generated.photos</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>im = cv2.imread("AI_no_makeup.jpg")
# Convert BGR image to RGB colorspace for a correct Matplotlib display. 
imDlib = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)
plt.imshow(imDlib)
</code></pre></div></div>
<p align="center"><img src="/OpenCV/data/images/AI_no_makeup.jpg" alt="" /></p>

<p>Load Dlibâ€™s face detector and the pre-trained 68-Point face landmark model. You can download the model file from Dlib website.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Initiate the face detector instance
faceDetector = dlib.get_frontal_face_detector()
# The landmark detector is implemented in the shape_predictor class
landmarkDetector = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
</code></pre></div></div>

<p>Detect faces in the image. The output of the Dlib face detector is a rectangle (x, y, w, h) that contains the face.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>faceRects = faceDetector(imDlib, 0)
</code></pre></div></div>

<p>Next step is to detect the 68 landmarks inside the detected face rectangle.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># To store landmark points
points = []
# For this simple application,
# I choose the first detected face for landmark detection. 
# Multiple detected faces situation isn't considered.
if len(faceRects) &gt; 0:
    newRect = dlib.rectangle(int(faceRects[0].left()),
                             int(faceRects[0].top()),
                             int(faceRects[0].right()),
                             int(faceRects[0].bottom()))
    # Detect landmarks
    landmarks = landmarkDetector(imDlib, newRect)

    # Convert Dlib shape detector object to list of tuples and store them.
    for p in landmarks.parts():
        pt = (p.x, p.y)
        points.append(pt)
else:
    print('No face detected')
</code></pre></div></div>

<p>The image below shows the detected 68 landmarks and their corresponding indices.</p>

<p align="center"><img src="/OpenCV/data/images/face_with_landmarks.jpg" alt="" /></p>

<p>I use the following equation to find the approximate locations of the left and right earlobe centres. This is just a rough calculation for this simple application, you can develop more accurate algorithms.</p>

<ul>
  <li>Left: (x coordinate of landmark point 0, y coordinate of the middle point between point 2 and 3)</li>
  <li>Right: (x coordinate of landmark point 16, y coordinate of the middle point between point 13 and 14)</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>leftLobeCentre = (points[0][0], int(points[2][1]+(points[3][1]-points[2][1])/2))
rightLobeCentre = (points[16][0], int(points[13][1]+(points[14][1]-points[13][1])/2))
</code></pre></div></div>

<p>Now read the left and right earring images. Note they are PNG images which contain R, G, B channels, as well as a 4th channel which is a transparent alpha mask.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Read the left earring image with alpha channel
lEaringIm = cv2.imread("left_earring.png", -1)
# Split png left earring image
bl,gl,rl,al = cv2.split(lEaringIm)
# Merge and convert RGB channels into an RGB image
lEaringIm = cv2.merge((bl,gl,rl))
lEaringIm = cv2.cvtColor(lEaringIm, cv2.COLOR_BGR2RGB)
# Save the alpha information into a single mask image
lAlpha = cv2.merge((al,al,al))

# Repeat the previous steps for the right earring PNG image
rEaringIm = cv2.imread("right_earring.png", -1)
br,gr,rr,ar = cv2.split(rEaringIm)
rEaringIm = cv2.merge((br,gr,rr))
rEaringIm = cv2.cvtColor(rEaringIm, cv2.COLOR_BGR2RGB)
rAlpha = cv2.merge((ar,ar,ar))
</code></pre></div></div>

<p>The earring images and their alpha masks:</p>

<p align="center"><img src="/OpenCV/data/images/left_earring.png" alt="" width="200" />  <img src="/OpenCV/data/images/left_earring_alpha.jpg" alt="" width="200" /> <img src="/OpenCV/data/images/right_earring.png" alt="" width="200" />  <img src="/OpenCV/data/images/right_earring_alpha.jpg" alt="" width="200" /></p>

<p>Resize the earring image height to 0.8 * height difference between landmark point 2 and 4. Then resize the earring width using the same scale. Note: This scale is based on best fitting results for the current earring images. You may use a different scale for your own earring images.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>h = int((points[4][1] - points[2][1])*0.8)
w = int(lEaringIm.shape[1]*h/lEaringIm.shape[0])
lEaringImResized = cv2.resize(lEaringIm, (w, h))
rEaringImResized = cv2.resize(rEaringIm, (w, h))

lAlphaResized = cv2.resize(lAlpha, (w, h))
rAlphaResized = cv2.resize(rAlpha, (w, h))
</code></pre></div></div>

<p>Find two rectangle regions of the exact size as the scaled earring images on the face image. The previously calculated earlobe centres are used as the top middle points of the regions.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Make a copy of the original image
imEarrings = imDlib.copy()

lReg = imEarrings[points[2][1]:points[2][1]+h, int(points[0][0]-w/2):int(points[0][0]-w/2)+w]
rReg = imEarrings[points[14][1]:points[14][1]+h, int(points[16][0]-w/2):int(points[16][0]-w/2)+w]
</code></pre></div></div>

<p>Perform alpha blending for the left and right earring regions. For more detailed information about Alpha Blending, go to <a href="https://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/">https://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Convert uint8 to float and then normalize the alpha mask to keep intensity between 0 and 1
lEaringImResized = lEaringImResized.astype(float)
lReg = lReg.astype(float)
lAlphaResized = lAlphaResized.astype(float)/255

rEaringImResized = rEaringImResized.astype(float)
rReg = rReg.astype(float)
rAlphaResized = rAlphaResized.astype(float)/255

# Perform alpha blending
lForeground = cv2.multiply(lAlphaResized, lEaringImResized)
lBackground = cv2.multiply(1.0 - lAlphaResized, lReg)
lOutImage = cv2.add(lForeground, lBackground)

rForeground = cv2.multiply(rAlphaResized, rEaringImResized)
rBackground = cv2.multiply(1.0 - rAlphaResized, rReg)
rOutImage = cv2.add(rForeground, rBackground)
</code></pre></div></div>

<p>Final step is to replace the earring regions in the original image with the alpha blended images.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>imEarrings[points[2][1]:points[2][1]+h, int(points[0][0]-w/2):int(points[0][0]-w/2)+w] = lOutImage
imEarrings[points[14][1]:points[14][1]+h, int(points[16][0]-w/2):int(points[16][0]-w/2)+w] = rOutImage
</code></pre></div></div>

<h2 id="final-results">Final Results</h2>

<p align="center"><img src="/OpenCV/data/images/face_with_earrings.jpg" alt="" /></p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/teapython/OpenCV">OpenCV</a> is maintained by <a href="https://github.com/teapython">teapython</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
